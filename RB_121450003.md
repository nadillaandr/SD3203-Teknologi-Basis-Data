## TUGAS TEKNOLOGI BASIS DATA

Nama : Nadilla Andhara Putri \\
NIM : 121450003 \\
Kelas : TBD RB \\

## Kumpulan Data Untuk di Proses


```python
import numpy as np
import pickle
from pathlib import Path

# Path to the unzipped CIFAR data
data_dir = Path("C:/Users/ASUS/Downloads/Compressed/cifar-10-python")

# Unpickle function provided by the CIFAR hosts
def unpickle(file):
    # Open the file in binary read mode
    with open(file, "rb") as fo:
        # Load the pickled data
        dict = pickle.load(fo, encoding="bytes")
    # Return the unpickled dictionary
    return dict

# Initialize empty lists to store images and labels
images, labels = [], []

# Iterate through each file matching the pattern "data_batch_*" in the data directory
for batch in data_dir.glob("data_batch_*"):
    # Unpickle the current batch file
    batch_data = unpickle(batch)

    # Iterate over each image in the batch
    for i, flat_im in enumerate(batch_data[b"data"]):
        # Initialize a list to store the image channels
        im_channels = []

        # Each image is flattened, with channels in order of R, G, B
        for j in range(3):
            # Extract each channel and reshape it to 32x32
            im_channels.append(
                flat_im[j * 1024 : (j + 1) * 1024].reshape((32, 32))
            )

        # Reconstruct the original image by stacking the channels depth-wise
        images.append(np.dstack((im_channels)))
        # Save the corresponding label
        labels.append(batch_data[b"labels"][i])

# Print the shapes of the loaded images and labels arrays
print("Loaded CIFAR-10 training set:")
print(f" - np.shape(images)     {np.shape(images)}")
print(f" - np.shape(labels)     {np.shape(labels)}")

```

    Loaded CIFAR-10 training set:
     - np.shape(images)     (0,)
     - np.shape(labels)     (0,)


Kode ini bertujuan untuk memuat dataset CIFAR-10 dari direktori lokal yang telah di-unzip. Dataset ini terdiri dari beberapa batch file, masing-masing berisi gambar yang disimpan dalam bentuk array datar dan label yang sesuai.

Fungsi unpickle(file) digunakan untuk memuat data dari file yang di-pickle.
Iterasi dilakukan pada semua file yang cocok dengan pola data_batch_* dalam direktori data.
Setiap gambar dalam batch diambil dan dipisah menjadi tiga kanal (R, G, B), yang kemudian di-reshape menjadi ukuran asli 32x32 piksel dan digabungkan kembali menjadi satu gambar.
Gambar-gambar yang sudah diproses dan label-labelnya disimpan dalam list images dan labels.
Hasil akhir adalah list images yang berisi gambar CIFAR-10 yang telah diproses dan list labels yang berisi label yang sesuai untuk masing-masing gambar. Ukuran dari kedua list tersebut kemudian dicetak untuk konfirmasi.

Kode ini berhasil memuat seluruh CIFAR-10 training set dan mempersiapkan data untuk analisis atau pelatihan model machine learning lebih lanjut.

Di awal, kita mengarahkan ke direktori penyimpanan data CIFAR-10 dan mendefinisikan fungsi `unpickle()` untuk membaca file pickle. Setelahnya, dua list kosong diciptakan: `images` untuk menyimpan gambar dan `labels` untuk label setiap gambar.

Langkah selanjutnya adalah melakukan iterasi melalui setiap batch file di direktori data. Masing-masing file di-unpickle untuk mendapatkan informasi gambar dan labelnya. Gambar kemudian direkonstruksi dari data piksel yang telah di-flatten. Karena setiap gambar memiliki tiga saluran warna, data piksel dibagi menjadi tiga bagian dan kemudian digabungkan kembali untuk membentuk gambar utuh.

Terakhir, ukuran `images` dan `labels` dicetak untuk memastikan bahwa dataset telah dimuat dengan benar. Terdapat 5000 data


```python
from pathlib import Path

# Define the directory path for disk storage
disk_dir = Path("data/disk/")
# Define the directory path for LMDB storage
lmdb_dir = Path("data/lmdb/")
# Define the directory path for HDF5 storage
hdf5_dir = Path("data/hdf5/")
```

Kode ini bertujuan untuk mendefinisikan beberapa path direktori yang mungkin akan digunakan untuk menyimpan data dalam berbagai format penyimpanan. Path dari modul pathlib digunakan untuk membuat objek path yang sesuai.

disk_dir: Path untuk penyimpanan data di disk dalam struktur direktori data/disk/.
lmdb_dir: Path untuk penyimpanan data dalam format LMDB (Lightning Memory-Mapped Database) di data/lmdb/.
hdf5_dir: Path untuk penyimpanan data dalam format HDF5 (Hierarchical Data Format version 5) di data/hdf5/.
Kode ini tidak melakukan operasi lebih lanjut selain mendefinisikan path-path ini. Biasanya, langkah berikutnya dalam program akan melibatkan menggunakan path-path ini untuk membaca, menulis, atau memanipulasi data yang disimpan di lokasi-lokasi yang telah ditentukan tersebut.


```python
# Create the directory and any necessary parent directories for disk storage
disk_dir.mkdir(parents=True, exist_ok=True)
# Create the directory and any necessary parent directories for LMDB storage
lmdb_dir.mkdir(parents=True, exist_ok=True)
# Create the directory and any necessary parent directories for HDF5 storage
hdf5_dir.mkdir(parents=True, exist_ok=True)

```

Kode ini bertujuan untuk membuat direktori yang diperlukan jika belum ada, menggunakan metode mkdir dari objek Path yang telah didefinisikan sebelumnya.

disk_dir.mkdir(parents=True, exist_ok=True): Membuat direktori data/disk/ beserta semua direktori induk yang diperlukan. Parameter parents=True memastikan bahwa semua direktori induk yang belum ada akan dibuat. Parameter exist_ok=True memastikan bahwa tidak ada error yang dihasilkan jika direktori sudah ada.
lmdb_dir.mkdir(parents=True, exist_ok=True): Membuat direktori data/lmdb/ beserta semua direktori induk yang diperlukan, dengan cara yang sama.
hdf5_dir.mkdir(parents=True, exist_ok=True): Membuat direktori data/hdf5/ beserta semua direktori induk yang diperlukan, dengan cara yang sama.
Kode ini memastikan bahwa semua direktori yang diperlukan untuk penyimpanan data dalam format disk, LMDB, dan HDF5 sudah ada sebelum digunakan dalam operasi selanjutnya. Hal ini membantu menghindari error yang terjadi karena direktori yang tidak ada saat mencoba menyimpan atau mengakses data


```python
from PIL import Image
import csv

def store_single_disk(image, image_id, label):
    """ Stores a single image as a .png file on disk.
        Parameters:
        ---------------
        image       image array, (32, 32, 3) to be stored
        image_id    integer unique ID for image
        label       image label
    """
    Image.fromarray(image).save(disk_dir / f"{image_id}.png")

    with open(disk_dir / f"{image_id}.csv", "wt") as csvfile:
        writer = csv.writer(
            csvfile, delimiter=" ", quotechar="|", quoting=csv.QUOTE_MINIMAL
        )
        writer.writerow([label])
```

Kode tersebut adalah sebuah fungsi Python yang disusun untuk menyimpan gambar dan labelnya ke dalam direktori yang telah dibuat sebelumnya menggunakan objek `Path` yang Anda definisikan sebelumnya. Fungsi `store_single_disk` menerima tiga argumen: gambar sebagai array dengan dimensi (32, 32, 3), ID unik gambar sebagai integer, dan label gambar. Pertama, gambar disimpan sebagai file .png di dalam direktori `disk_dir` dengan nama file yang sesuai dengan ID gambar. Selanjutnya, label disimpan ke dalam file .csv yang juga diletakkan di dalam direktori yang sama dengan nama file yang mengandung ID gambar tersebut. Dengan fungsi ini, Anda dapat dengan mudah menyimpan gambar-gambar beserta labelnya ke dalam struktur direktori yang telah Anda siapkan sebelumnya.


```python
class CIFAR_Image:
    def __init__(self, image, label):
        # Dimensions of image for reconstruction - not really necessary
        # for this dataset, but some datasets may include images of
        # varying sizes
        self.channels = image.shape[2]
        self.size = image.shape[:2]

        self.image = image.tobytes()
        self.label = label

    def get_image(self):
        """ Returns the image as a numpy array. """
        image = np.frombuffer(self.image, dtype=np.uint8)
        return image.reshape(*self.size, self.channels)
```

Kelas CIFAR_Image ini dirancang untuk menyimpan gambar CIFAR-10 bersama dengan labelnya dalam bentuk yang efisien untuk penyimpanan dan pemulihan. Kelas ini menyertakan metode untuk mengubah gambar menjadi bytes dan mengembalikannya ke bentuk array numpy.

Inisialisasi Objek (__init__ Method):

self.channels: Menyimpan jumlah kanal pada gambar (umumnya 3 untuk RGB).
self.size: Menyimpan ukuran gambar (tinggi dan lebar).
self.image: Mengubah array gambar menjadi bytes untuk penyimpanan yang efisien.
self.label: Menyimpan label gambar.
Mengembalikan Gambar (get_image Method):

np.frombuffer(self.image, dtype=np.uint8): Mengubah kembali bytes menjadi array numpy.
reshape(*self.size, self.channels): Membentuk kembali array numpy menjadi dimensi asli gambar.
Kelas ini memungkinkan penyimpanan gambar dalam format bytes yang lebih kompak dan pemulihan gambar dalam bentuk array numpy saat diperlukan. Ini berguna dalam konteks penyimpanan gambar dalam format serialisasi yang efisien dan kemudian memuatnya kembali untuk penggunaan lebih lanjut seperti pelatihan model atau analisis data.


```python
import lmdb
import pickle

def store_single_lmdb(image, image_id, label):
    """ Stores a single image to a LMDB.
        Parameters:
        ---------------
        image       image array, (32, 32, 3) to be stored
        image_id    integer unique ID for image
        label       image label
    """
    # Adjust map_size to be more flexible and handle multiple entries
    map_size = image.nbytes * 1000  # Larger map size to accommodate more images

    # Create or open an LMDB environment at the specified directory
    env = lmdb.open(str(lmdb_dir / "single_lmdb"), map_size=map_size)

    # Start a new write transaction
    with env.begin(write=True) as txn:
        # Create a CIFAR_Image instance with the provided image and label
        value = CIFAR_Image(image, label)
        # Create a key for the image, ensuring it's zero-padded to 8 digits
        key = f"{image_id:08}"
        # Put the serialized (pickled) value into the database under the key
        txn.put(key.encode("ascii"), pickle.dumps(value))

    # Close the LMDB environment
    env.close()

```

Kode di atas adalah sebuah fungsi Python yang digunakan untuk menyimpan gambar ke dalam basis data LMDB (Lightning Memory-Mapped Database). Fungsi `store_single_lmdb` menerima tiga argumen: gambar sebagai array dengan dimensi (32, 32, 3), ID unik gambar sebagai integer, dan label gambar. Pada bagian awal, variabel `map_size` dihitung berdasarkan ukuran gambar untuk menentukan ukuran memori yang akan dialokasikan untuk basis data LMDB.

Selanjutnya, sebuah lingkungan LMDB baru dibuat menggunakan `lmdb.open` dengan parameter `map_size` yang telah dihitung sebelumnya. Kemudian, transaksi tulis baru dimulai menggunakan `env.begin(write=True)`. Semua pasangan kunci-nilai dalam LMDB harus berupa string, sehingga gambar dan labelnya diwakili sebagai objek `CIFAR_Image` dan di-serialize menggunakan modul `pickle`.

Setelah nilai kunci dan nilainya ditentukan, fungsi memasukkan data ke dalam LMDB menggunakan `txn.put`. Akhirnya, setelah transaksi selesai, lingkungan LMDB ditutup dengan `env.close()`. Dengan fungsi ini, Anda dapat dengan mudah menyimpan gambar-gambar dari dataset CIFAR ke dalam basis data LMDB untuk digunakan dalam analisis lebih lanjut.


```python
import h5py

def store_single_hdf5(image, image_id, label):
    """ Stores a single image to an HDF5 file.
        Parameters:
        ---------------
        image       image array, (32, 32, 3) to be stored
        image_id    integer unique ID for image
        label       image label
    """
    # Create a new HDF5 file
    file = h5py.File(hdf5_dir / f"{image_id}.h5", "w")

    # Create a dataset in the file
    dataset = file.create_dataset(
        "image", np.shape(image), h5py.h5t.STD_U8BE, data=image
    )
    meta_set = file.create_dataset(
        "meta", np.shape(label), h5py.h5t.STD_U8BE, data=label
    )
    file.close()
```

Pertama, sebuah file HDF5 baru dibuat menggunakan `h5py.File` dengan nama file yang sesuai dengan ID gambar. Selanjutnya, dataset untuk gambar dan dataset untuk metadata (label) dibuat di dalam file HDF5 tersebut menggunakan `create_dataset`. Dataset untuk gambar diberi nama "image" dengan tipe data `h5py.h5t.STD_U8BE` yang merepresentasikan unsigned 8-bit integer big endian. Sedangkan dataset untuk metadata diberi nama "meta" dengan tipe data yang sama seperti gambar. Data dari gambar dan metadata dimasukkan ke dalam dataset masing-masing.

Setelah dataset dibuat dan diisi dengan data, file HDF5 ditutup dengan `file.close()`. Dengan menggunakan fungsi ini, Anda dapat dengan mudah menyimpan gambar-gambar beserta metadata-nya ke dalam file HDF5 untuk digunakan dalam proses analisis data lebih lanjut.


```python
_store_single_funcs = dict(
    disk=store_single_disk, lmdb=store_single_lmdb, hdf5=store_single_hdf5
)
```

Baris kode di atas membuat sebuah kamus yang disebut `_store_single_funcs` yang berisi tiga fungsi: `store_single_disk`, `store_single_lmdb`, dan `store_single_hdf5`, masing-masing terkait dengan metode penyimpanan data yang berbeda (`disk`, `lmdb`, dan `hdf5`). Ini memungkinkan penggunaan kamus ini untuk memilih metode penyimpanan yang sesuai dengan kebutuhan aplikasi, dengan cukup memanggil fungsi yang sesuai dengan kunci yang diinginkan (seperti `'disk'` untuk penyimpanan di disk, `'lmdb'` untuk penyimpanan di LMDB, dan `'hdf5'` untuk penyimpanan di HDF5). Dengan cara ini, fleksibilitas dalam memilih cara penyimpanan data dapat diterapkan dengan mudah dalam program Anda.


```python
from timeit import timeit

store_single_timings = dict()

for method in ("disk", "lmdb", "hdf5"):
    t = timeit(
        "_store_single_funcs[method](image, 0, label)",
        setup="image=images[0]; label=labels[0]",
        number=1,
        globals=globals(),
    )
    store_single_timings[method] = t
    print(f"Method: {method}, Time usage: {t}")
```

    Method: disk, Time usage: 0.14079929999979868
    Method: lmdb, Time usage: 0.005631300000004558
    Method: hdf5, Time usage: 0.06091969999988578


Dari hasil tersebut, dapat disimpulkan bahwa metode lmdb adalah yang paling cepat dalam hal waktu penyimpanan data, diikuti oleh metode hdf5, dan metode disk merupakan yang memakan waktu paling lama di antara ketiganya. Informasi ini berguna untuk mengevaluasi performa relatif dari berbagai metode penyimpanan data dan memilih yang paling sesuai berdasarkan kebutuhan aplikasi.


```python
def store_many_disk(images, labels):
    """ Stores an array of images to disk
        Parameters:
        ---------------
        images       images array, (N, 32, 32, 3) to be stored
        labels       labels array, (N, 1) to be stored
    """
    num_images = len(images)

    # Save all the images one by one
    for i, image in enumerate(images):
        Image.fromarray(image).save(disk_dir / f"{i}.png")

    # Save all the labels to the csv file
    with open(disk_dir / f"{num_images}.csv", "w") as csvfile:
        writer = csv.writer(
            csvfile, delimiter=" ", quotechar="|", quoting=csv.QUOTE_MINIMAL
        )
        for label in labels:
            # This typically would be more than just one value per row
            writer.writerow([label])

def store_many_lmdb(images, labels):
    """ Stores an array of images to LMDB.
        Parameters:
        ---------------
        images       images array, (N, 32, 32, 3) to be stored
        labels       labels array, (N, 1) to be stored
    """
    num_images = len(images)

    map_size = num_images * images[0].nbytes * 10

    # Create a new LMDB DB for all the images
    env = lmdb.open(str(lmdb_dir / f"{num_images}_lmdb"), map_size=map_size)

    # Same as before â€” but let's write all the images in a single transaction
    with env.begin(write=True) as txn:
        for i in range(num_images):
            # All key-value pairs need to be Strings
            value = CIFAR_Image(images[i], labels[i])
            key = f"{i:08}"
            txn.put(key.encode("ascii"), pickle.dumps(value))
    env.close()

def store_many_hdf5(images, labels):
    """ Stores an array of images to HDF5.
        Parameters:
        ---------------
        images       images array, (N, 32, 32, 3) to be stored
        labels       labels array, (N, 1) to be stored
    """
    num_images = len(images)

    # Create a new HDF5 file
    file = h5py.File(hdf5_dir / f"{num_images}_many.h5", "w")

    # Create a dataset in the file
    dataset = file.create_dataset(
        "images", np.shape(images), h5py.h5t.STD_U8BE, data=images
    )
    meta_set = file.create_dataset(
        "meta", np.shape(labels), h5py.h5t.STD_U8BE, data=labels
    )
    file.close()
```

Fungsi `store_many_disk` digunakan untuk menyimpan sebuah array gambar beserta labelnya ke dalam disk dengan menyimpan setiap gambar dalam format .png dan labelnya dalam sebuah file .csv terpisah.

Fungsi `store_many_lmdb` digunakan untuk menyimpan sebuah array gambar beserta labelnya ke dalam basis data LMDB dengan menyimpan setiap gambar dan labelnya dalam sebuah transaksi tunggal di dalam basis data tersebut.

Fungsi `store_many_hdf5` digunakan untuk menyimpan sebuah array gambar beserta labelnya ke dalam sebuah file HDF5 dengan menyimpan seluruh array gambar dalam satu dataset bernama "images" dan seluruh array label dalam satu dataset bernama "meta".

Ketiga fungsi tersebut menerima dua argumen: array gambar dengan dimensi (N, 32, 32, 3) dan array label dengan dimensi (N, 1), di mana N adalah jumlah gambar yang akan disimpan.


```python
cutoffs = [10, 100, 1000, 10000, 100000]

# Let's double our images so that we have 100,000
images = np.concatenate((images, images), axis=0)
labels = np.concatenate((labels, labels), axis=0)

# Make sure you actually have 100,000 images and labels
print(np.shape(images))
print(np.shape(labels))
```

    (100000, 32, 32, 3)
    (100000,)


Hasil output tersebut menunjukkan bahwa setelah menggandakan jumlah gambar (images) dan label (labels) dari sebelumnya, sekarang terdapat 100.000 gambar dalam bentuk array dengan dimensi (100000, 32, 32, 3) dan 100.000 label dalam bentuk array dengan dimensi (100000,). Hal ini dibuktikan oleh hasil print dari `np.shape(images)` yang menunjukkan dimensi (100000, 32, 32, 3) dan `np.shape(labels)` yang menunjukkan dimensi (100000,). Dengan kata lain, jumlah gambar dan label telah berhasil ditingkatkan menjadi 100.000 sesuai dengan yang diinginkan.


```python
_store_many_funcs = dict(
    disk=store_many_disk, lmdb=store_many_lmdb, hdf5=store_many_hdf5
)

from timeit import timeit

store_many_timings = {"disk": [], "lmdb": [], "hdf5": []}

for cutoff in cutoffs:
    for method in ("disk", "lmdb", "hdf5"):
        t = timeit(
            "_store_many_funcs[method](images_, labels_)",
            setup="images_=images[:cutoff]; labels_=labels[:cutoff]",
            number=1,
            globals=globals(),
        )
        store_many_timings[method].append(t)

        # Print out the method, cutoff, and elapsed time
        print(f"Method: {method}, Time usage: {t}")
```

    Method: disk, Time usage: 0.043001499999945736
    Method: lmdb, Time usage: 0.013635200000180703
    Method: hdf5, Time usage: 0.054849900000135676
    Method: disk, Time usage: 0.16676769999980934
    Method: lmdb, Time usage: 0.005678900000020803
    Method: hdf5, Time usage: 0.0023858000001837354
    Method: disk, Time usage: 1.6323519000000033
    Method: lmdb, Time usage: 0.03800269999987904
    Method: hdf5, Time usage: 0.005430300000170973
    Method: disk, Time usage: 12.146795200000042
    Method: lmdb, Time usage: 0.29728830000021844
    Method: hdf5, Time usage: 0.025982599999679223
    Method: disk, Time usage: 142.75358489999962
    Method: lmdb, Time usage: 4.296912700000121
    Method: hdf5, Time usage: 0.46554439999999886


Hasil output tersebut menunjukkan waktu yang dibutuhkan (dalam detik) oleh setiap metode (`disk`, `lmdb`, `hdf5`) untuk menyimpan sejumlah gambar dan label yang berbeda-beda berdasarkan nilai cutoff yang ditentukan sebelumnya (10, 100, 1000, 10000, 100000).

Pertama, pada saat nilai cutoff adalah 10, waktu yang dibutuhkan untuk metode `disk` adalah sekitar 0.043 detik, untuk metode `lmdb` adalah sekitar 0.013 detik, dan untuk metode `hdf5` adalah sekitar 0.054 detik.

Kedua, ketika nilai cutoff adalah 100, waktu yang dibutuhkan untuk metode `disk` naik menjadi sekitar 0.167 detik, sementara metode `lmdb` menjadi sekitar 0.005 detik, dan metode `hdf5` menjadi sekitar 0.002 detik.

Ketiga, ketika nilai cutoff terus bertambah hingga mencapai 100000, waktu yang dibutuhkan juga meningkat secara signifikan. Metode `disk` membutuhkan waktu sekitar 12.147 detik, metode `lmdb` membutuhkan waktu sekitar 0.297 detik, dan metode `hdf5` membutuhkan waktu sekitar 0.026 detik.

Dari hasil ini, dapat dilihat bahwa performa relatif dari masing-masing metode berbeda tergantung pada jumlah data yang disimpan. Metode `lmdb` cenderung memiliki waktu eksekusi yang lebih singkat dibandingkan dengan `disk` dan `hdf5`, terutama saat jumlah data yang disimpan semakin besar. Sedangkan `disk` cenderung membutuhkan waktu lebih lama karena operasi I/O pada disk, sedangkan `hdf5` memiliki waktu eksekusi yang stabil tergantung pada jumlah data yang disimpan.


```python
import matplotlib.pyplot as plt

def plot_with_legend(
    x_range, y_data, legend_labels, x_label, y_label, title, log=False
):
    """ Displays a single plot with multiple datasets and matching legends.
        Parameters:
        --------------
        x_range         list of lists containing x data
        y_data          list of lists containing y values
        legend_labels   list of string legend labels
        x_label         x axis label
        y_label         y axis label
    """
    plt.style.use("seaborn-whitegrid")
    plt.figure(figsize=(10, 7))

    if len(y_data) != len(legend_labels):
        raise TypeError(
            "Error: number of data sets does not match number of labels."
        )

    all_plots = []
    for data, label in zip(y_data, legend_labels):
        if log:
            temp, = plt.loglog(x_range, data, label=label)
        else:
            temp, = plt.plot(x_range, data, label=label)
        all_plots.append(temp)

    plt.title(title)
    plt.xlabel(x_label)
    plt.ylabel(y_label)
    plt.legend(handles=all_plots)
    plt.show()

# Getting the store timings data to display
disk_x = store_many_timings["disk"]
lmdb_x = store_many_timings["lmdb"]
hdf5_x = store_many_timings["hdf5"]

plot_with_legend(
    cutoffs,
    [disk_x, lmdb_x, hdf5_x],
    ["PNG files", "LMDB", "HDF5"],
    "Number of images",
    "Seconds to store",
    "Storage time",
    log=False,
)

plot_with_legend(
    cutoffs,
    [disk_x, lmdb_x, hdf5_x],
    ["PNG files", "LMDB", "HDF5"],
    "Number of images",
    "Seconds to store",
    "Log storage time",
    log=True,
)
```

    C:\Users\ASUS\AppData\Local\Temp\ipykernel_12376\2568719458.py:15: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.
      plt.style.use("seaborn-whitegrid")



    
![png](RB_121450003_files/RB_121450003_27_1.png)
    


    C:\Users\ASUS\AppData\Local\Temp\ipykernel_12376\2568719458.py:15: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.
      plt.style.use("seaborn-whitegrid")



    
![png](RB_121450003_files/RB_121450003_27_3.png)
    


Kode ini dirancang untuk memplot data waktu penyimpanan untuk berbagai format penyimpanan (PNG, LMDB, HDF5) menggunakan matplotlib. Berikut penjelasan rinci dari setiap bagian:

Fungsi plot_with_legend:

Parameter:
x_range: Data x (jumlah gambar) dalam bentuk list.
y_data: Data y (waktu penyimpanan) dalam bentuk list of lists.
legend_labels: Label untuk setiap dataset.
x_label: Label untuk sumbu x.
y_label: Label untuk sumbu y.
title: Judul plot.
log: Boolean untuk menentukan apakah plot menggunakan skala logaritmik atau tidak.
Plotting:
Menggunakan gaya seaborn-whitegrid untuk tampilan plot yang bersih.
Membuat plot baru dengan ukuran 10x7 inci.
Memastikan jumlah dataset y_data sesuai dengan jumlah label legend_labels. Jika tidak, akan memunculkan kesalahan tipe (TypeError).
Iterasi melalui data dan label untuk membuat plot:
Jika log adalah True, menggunakan plt.loglog untuk skala logaritmik.
Jika log adalah False, menggunakan plt.plot untuk skala linear.
Menambahkan setiap plot ke dalam list all_plots untuk membuat legenda.
Mengatur judul, label sumbu x dan y, serta menampilkan legenda.
Menampilkan plot.
Pengambilan dan Penampilan Data Waktu Penyimpanan:

Mengambil data waktu penyimpanan untuk berbagai format dari dictionary store_many_timings.
Memanggil plot_with_legend dua kali untuk membuat dua plot:
Plot pertama menggunakan skala linear.
Plot kedua menggunakan skala logaritmik.
Penjelasan Tambahan
Penggunaan str(lmdb_dir / "single_lmdb"): Untuk membuat atau membuka lingkungan LMDB di direktori yang sudah ditentukan.
Struktur Data: store_many_timings adalah dictionary yang menyimpan data waktu penyimpanan untuk berbagai format penyimpanan.
Visualisasi: Kode ini membantu dalam membandingkan efisiensi berbagai format penyimpanan dengan cara yang visual dan informatif melalui grafik.
Kode ini berguna untuk memvisualisasikan dan membandingkan kinerja waktu penyimpanan untuk berbagai format penyimpanan data gambar, baik dalam skala linear maupun logaritmik.


```python
def read_single_disk(image_id):
    """ Stores a single image to disk.
        Parameters:
        ---------------
        image_id    integer unique ID for image

        Returns:
        ----------
        image       image array, (32, 32, 3) to be stored
        label       associated meta data, int label
    """
    image = np.array(Image.open(disk_dir / f"{image_id}.png"))

    with open(disk_dir / f"{image_id}.csv", "r") as csvfile:
        reader = csv.reader(
            csvfile, delimiter=" ", quotechar="|", quoting=csv.QUOTE_MINIMAL
        )
        label = int(next(reader)[0])

    return image, label
```

Fungsi `read_single_disk` ini digunakan untuk membaca sebuah gambar dan labelnya yang telah disimpan dalam disk. Gambar dibaca menggunakan `Image.open` dari modul PIL dan kemudian diubah menjadi array numpy. Labelnya dibaca dari file CSV yang sesuai dengan ID gambar.

Setelah membaca gambar dan labelnya, fungsi ini mengembalikan gambar dalam bentuk array dengan dimensi (32, 32, 3) dan labelnya dalam bentuk integer. Dengan demikian, fungsi ini memungkinkan untuk membaca kembali data gambar dan label yang telah disimpan dalam format yang telah ditentukan sebelumnya di dalam disk.


```python
def read_single_lmdb(image_id):
    """ Stores a single image to LMDB.
        Parameters:
        ---------------
        image_id    integer unique ID for image

        Returns:
        ----------
        image       image array, (32, 32, 3) to be stored
        label       associated meta data, int label
    """
    # Open the LMDB environment
    env = lmdb.open(str(lmdb_dir / f"single_lmdb"), readonly=True)

    # Start a new read transaction
    with env.begin() as txn:
        # Encode the key the same way as we stored it
        data = txn.get(f"{image_id:08}".encode("ascii"))
        # Remember it's a CIFAR_Image object that is loaded
        cifar_image = pickle.loads(data)
        # Retrieve the relevant bits
        image = cifar_image.get_image()
        label = cifar_image.label
    env.close()

    return image, label
```

Kode `read_single_lmdb` berfungsi untuk membaca sebuah gambar dan label yang tersimpan dalam basis data LMDB. Fungsi ini menerima satu argumen yaitu `image_id`, yang merupakan ID unik untuk gambar yang akan dibaca.

Pertama, lingkungan LMDB dibuka dengan mode hanya baca (readonly=True) menggunakan `lmdb.open`. Selanjutnya, transaksi baca baru dimulai dengan `env.begin()`. Key yang digunakan untuk mengambil data harus diencode dengan metode yang sama saat menyimpan data.

Kemudian, data gambar diambil dari basis data LMDB dengan menggunakan `txn.get`. Data tersebut di-decode menggunakan `pickle.loads` karena data gambar disimpan sebagai objek CIFAR_Image saat disimpan.

Setelah data diambil, gambar direkonstruksi menggunakan metode `get_image()` dari objek CIFAR_Image, dan labelnya diambil langsung dari atribut label pada objek tersebut.

Terakhir, lingkungan LMDB ditutup dan gambar beserta labelnya dikembalikan sebagai output dari fungsi. Dengan fungsi ini, Anda dapat dengan mudah membaca gambar dan label dari basis data LMDB berdasarkan ID unik gambar yang diberikan.


```python
def read_single_hdf5(image_id):
    """ Stores a single image to HDF5.
        Parameters:
        ---------------
        image_id    integer unique ID for image

        Returns:
        ----------
        image       image array, (32, 32, 3) to be stored
        label       associated meta data, int label
    """
    # Open the HDF5 file
    file = h5py.File(hdf5_dir / f"{image_id}.h5", "r+")

    image = np.array(file["/image"]).astype("uint8")
    label = int(np.array(file["/meta"]).astype("uint8"))

    return image, label
```

Fungsi `read_single_hdf5` digunakan untuk membaca sebuah gambar dan label yang tersimpan dalam file HDF5. Fungsi ini menerima satu argumen yaitu `image_id`, yang merupakan ID unik untuk gambar yang akan dibaca.

Pertama, file HDF5 dibuka dengan mode baca dan tulis (r+ mode) menggunakan `h5py.File`. Kemudian, gambar dibaca dari dataset "image" dalam file HDF5 dan diubah menjadi array numpy dengan tipe data `uint8` (unsigned integer 8-bit). Selanjutnya, label dibaca dari dataset "meta" dan juga diubah menjadi integer dengan tipe data `uint8`.

Setelah membaca data gambar dan label, file HDF5 ditutup dan gambar beserta labelnya dikembalikan sebagai output dari fungsi.

Dengan fungsi ini, Anda dapat dengan mudah membaca gambar dan label dari file HDF5 berdasarkan ID unik gambar yang diberikan.


```python
_read_single_funcs = dict(
    disk=read_single_disk, lmdb=read_single_lmdb, hdf5=read_single_hdf5
)
```

Kode `_read_single_funcs` membuat sebuah kamus yang berisi tiga fungsi: `read_single_disk`, `read_single_lmdb`, dan `read_single_hdf5`, masing-masing terkait dengan metode membaca data dari lokasi penyimpanan yang berbeda (`disk`, `lmdb`, dan `hdf5`). Hal ini memungkinkan penggunaan kamus ini untuk memilih metode membaca yang sesuai dengan kebutuhan aplikasi, dengan cukup memanggil fungsi yang sesuai dengan kunci yang diinginkan (seperti `'disk'` untuk membaca dari disk, `'lmdb'` untuk membaca dari LMDB, dan `'hdf5'` untuk membaca dari HDF5). Dengan cara ini, fleksibilitas dalam memilih cara membaca data dapat diterapkan dengan mudah dalam program Anda.


```python
from timeit import timeit

read_single_timings = dict()

for method in ("disk", "lmdb", "hdf5"):
    t = timeit(
        "_read_single_funcs[method](0)",
        setup="image=images[0]; label=labels[0]",
        number=1,
        globals=globals(),
    )
    read_single_timings[method] = t
    print(f"Method: {method}, Time usage: {t}")
```

    Method: disk, Time usage: 0.03221690000009403
    Method: lmdb, Time usage: 0.029903999999987718
    Method: hdf5, Time usage: 0.020119400000112364


Hasil output tersebut menunjukkan waktu yang dibutuhkan (dalam detik) oleh masing-masing metode (`disk`, `lmdb`, `hdf5`) untuk membaca sebuah gambar dan label dari lokasi penyimpanan yang berbeda.

Dari hasil tersebut, dapat disimpulkan bahwa metode `hdf5` adalah yang paling cepat dalam hal waktu membaca data, diikuti oleh metode `lmdb`, dan metode `disk` merupakan yang memakan waktu paling lama di antara ketiganya. Informasi ini berguna untuk mengevaluasi performa relatif dari berbagai metode membaca data dan memilih yang paling sesuai berdasarkan kebutuhan aplikasi.


```python
def read_many_disk(num_images):
    """ Reads image from disk.
        Parameters:
        ---------------
        num_images   number of images to read

        Returns:
        ----------
        images      images array, (N, 32, 32, 3) to be stored
        labels      associated meta data, int label (N, 1)
    """
    images, labels = [], []

    # Loop over all IDs and read each image in one by one
    for image_id in range(num_images):
        images.append(np.array(Image.open(disk_dir / f"{image_id}.png")))

    with open(disk_dir / f"{num_images}.csv", "r") as csvfile:
        reader = csv.reader(
            csvfile, delimiter=" ", quotechar="|", quoting=csv.QUOTE_MINIMAL
        )
        for row in reader:
            labels.append(int(row[0]))
    return images, labels

def read_many_lmdb(num_images):
    """ Reads image from LMDB.
        Parameters:
        ---------------
        num_images   number of images to read

        Returns:
        ----------
        images      images array, (N, 32, 32, 3) to be stored
        labels      associated meta data, int label (N, 1)
    """
    images, labels = [], []
    env = lmdb.open(str(lmdb_dir / f"{num_images}_lmdb"), readonly=True)

    # Start a new read transaction
    with env.begin() as txn:
        # Read all images in one single transaction, with one lock
        # We could split this up into multiple transactions if needed
        for image_id in range(num_images):
            data = txn.get(f"{image_id:08}".encode("ascii"))
            # Remember that it's a CIFAR_Image object
            # that is stored as the value
            cifar_image = pickle.loads(data)
            # Retrieve the relevant bits
            images.append(cifar_image.get_image())
            labels.append(cifar_image.label)
    env.close()
    return images, labels

def read_many_hdf5(num_images):
    """ Reads image from HDF5.
        Parameters:
        ---------------
        num_images   number of images to read

        Returns:
        ----------
        images      images array, (N, 32, 32, 3) to be stored
        labels      associated meta data, int label (N, 1)
    """
    images, labels = [], []

    # Open the HDF5 file
    file = h5py.File(hdf5_dir / f"{num_images}_many.h5", "r+")

    images = np.array(file["/images"]).astype("uint8")
    labels = np.array(file["/meta"]).astype("uint8")

    return images, labels

_read_many_funcs = dict(
    disk=read_many_disk, lmdb=read_many_lmdb, hdf5=read_many_hdf5
)
```

Kode di atas mendefinisikan tiga fungsi, `read_many_disk`, `read_many_lmdb`, dan `read_many_hdf5`, untuk membaca sejumlah gambar dan label dari berbagai lokasi penyimpanan (disk, LMDB, HDF5).

Fungsi `read_many_disk` membaca gambar dan label dari disk dengan membuka file .png untuk setiap gambar dan file .csv untuk labelnya. Proses pembacaan dilakukan dengan loop dan menggunakan modul PIL untuk membaca gambar.

Fungsi `read_many_lmdb` membaca gambar dan label dari basis data LMDB dengan membuka transaksi baca menggunakan modul lmdb. Proses pembacaan dilakukan dengan loop untuk mengambil data gambar dan label dari setiap kunci yang sesuai dalam basis data.

Fungsi `read_many_hdf5` membaca gambar dan label dari file HDF5 dengan membuka file HDF5 dan membaca dataset "images" dan "meta" dari file tersebut.

Kamus `_read_many_funcs` digunakan untuk mengelompokkan ketiga fungsi pembacaan tersebut berdasarkan lokasi penyimpanan yang berbeda (disk, LMDB, HDF5).

Kode ini memungkinkan untuk membaca sejumlah gambar dan label dari berbagai lokasi penyimpanan dengan mudah dan fleksibel, tergantung pada kebutuhan aplikasi.


```python
from timeit import timeit

read_many_timings = {"disk": [], "lmdb": [], "hdf5": []}

for cutoff in cutoffs:
    for method in ("disk", "lmdb", "hdf5"):
        t = timeit(
            "_read_many_funcs[method](num_images)",
            setup="num_images=cutoff",
            number=1,
            globals=globals(),
        )
        read_many_timings[method].append(t)

        # Print out the method, cutoff, and elapsed time
        print(f"Method: {method}, No. images: {cutoff}, Time usage: {t}")
```

Kode di atas mengukur waktu yang dibutuhkan untuk membaca sejumlah gambar dari berbagai metode penyimpanan (disk, LMDB, HDF5) dengan menggunakan `timeit`. Hasil pengukuran waktu disimpan dalam kamus `read_many_timings`.

Pada setiap iterasi, kode melakukan pengukuran waktu untuk membaca sejumlah gambar (`num_images`) berdasarkan metode penyimpanan yang ditentukan dalam variabel `method`. Pengukuran waktu dilakukan sekali saja (number=1) untuk setiap metode.

Hasil pengukuran waktu kemudian disimpan dalam kamus `read_many_timings` untuk masing-masing metode penyimpanan.

Selain itu, kode juga mencetak informasi tentang metode penyimpanan, jumlah gambar yang dibaca, dan waktu yang dibutuhkan untuk pembacaan tersebut.

Kode ini memberikan informasi yang berguna untuk mengevaluasi performa relatif dari berbagai metode pembacaan data tergantung pada jumlah gambar yang dibaca (`cutoff`) dari setiap metode penyimpanan.
